# Precision Farming Agent - Quick Reference

## ğŸš€ Quick Start

### Run the Agent
```bash
python src/main.py "What are the light requirements for tomato seedlings?"
```

### Run Tests
```bash
# Integration tests
python test_agent.py

# Performance benchmark
python benchmark.py

# Component tests
python test_scraper.py
python test_retriever.py
```

---

## ğŸ“ Project Structure

```
precision_farming/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â””â”€â”€ core.py              # Agent runtime with few-shot prompting
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ golden_examples.json # Expert recipe examples
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ scraper.py           # Web scraping tool
â”‚   â”‚   â””â”€â”€ retriever.py         # ChromaDB retrieval tool
â”‚   â””â”€â”€ main.py                  # Entry point
â”œâ”€â”€ test_agent.py                # Integration tests
â”œâ”€â”€ benchmark.py                 # Performance tests
â”œâ”€â”€ TESTING.md                   # Testing guide
â””â”€â”€ theory.md                    # Architecture documentation
```

---

## ğŸ¯ Key Features

### 1. Dynamic Few-Shot Prompting
- **5 golden examples** covering different crops
- **Cross-domain selection** prevents hallucination
- **Automatic example injection** based on query

### 2. Model Context Protocol (MCP)
- Context lock: Facts only from retrieved data
- Refusal behavior: Returns error if no data
- Source tracing: All facts cited

### 3. Schema Validation
- Pydantic ensures valid JSON output
- Required fields enforced
- Confidence scoring

---

## ğŸ“Š System Prompt Structure

```
## CRITICAL CONSTRAINTS
1. CONTEXT LOCK
2. NO PRIOR KNOWLEDGE
3. REFUSAL PRIORITY
4. SOURCE TRACING

## OUTPUT FORMAT
[Dynamic cross-domain example]

## INSTRUCTIONS
- Use structure from example
- Extract facts from context
- Be specific with numbers/units
```

---

## ğŸ§ª Test Queries

### âœ… Should Succeed
- "What are the light requirements for tomato seedlings?"
- "How should I water wheat during tillering?"
- "What fertilizer does corn need?"

### âŒ Should Refuse
- "How do I grow alien vegetables on Mars?"
- "What is the capital of France?"

---

## âš™ï¸ Configuration

### Agent Parameters (core.py)
```python
n_ctx=4096      # Context window
n_threads=4     # CPU threads (adjust to your CPU)
temperature=0.0 # Deterministic output
```

### Performance Tuning
- **Faster inference**: Increase `n_threads`, use Q3 quantization
- **Better quality**: Increase `n_ctx`, use Q4/Q5 quantization
- **Lower memory**: Decrease `n_ctx`, use Q3 quantization

---

## ğŸ“ˆ Expected Performance

| Metric | Target |
|--------|--------|
| Initialization | ~20-30s |
| Query Response | ~10-15s |
| Success Rate | >90% |

---

## ğŸ” Validation Checklist

- âœ… Schema compliance (all fields present)
- âœ… MCP enforcement (refusal when no data)
- âœ… Output specificity (numbers + units)
- âœ… Source citations included
- âœ… Confidence scores 0.0-1.0

---

## ğŸ› Troubleshooting

### Slow Inference
- Check `n_threads` matches CPU cores
- Try Q3 quantization
- Reduce `n_ctx` to 2048

### Generic Answers
- Verify golden examples loaded
- Check ChromaDB has data
- Review system prompt

### Schema Violations
- Update llama-cpp-python
- Check Pydantic version
- Review response_format support

---

## ğŸ“š Documentation

- **TESTING.md** - Comprehensive testing guide
- **theory.md** - Architecture and concepts
- **explanations.md** - Multi-audience explanations
- **walkthrough.md** - Implementation details

---

## ğŸ“ How Cross-Domain Prompting Works

1. User asks about **Tomato**
2. System detects "Tomato" in query
3. System selects **Corn** example (different crop)
4. LLM learns **structure** from Corn example
5. LLM extracts **facts** from Tomato context
6. Output: Tomato recipe with Corn-level detail

**Result**: Prevents hallucination while ensuring specificity!
